{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd.variable import Variable\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import imageio\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from data_utils.dataset import LSUNDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = '../data/data'\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 1\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 100\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "#nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 128\n",
    "\n",
    "# number of subsequent discriminator trainings\n",
    "k = 1\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 2\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = LSUNDataset('../data/images.txt', dataroot,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,),(0.5,))\n",
    "    ]))\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                        shuffle=True, num_workers=workers)\n",
    "\n",
    "# to image transform\n",
    "to_image = transforms.ToPILImage()\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        self.ngpu = ngpu\n",
    "        super(Generator, self).__init__()\n",
    "        self.n_features = nz\n",
    "        self.n_out = 64*64*1\n",
    "\n",
    "        self.fc0 = nn.Sequential(\n",
    "            nn.Linear(self.n_features, 256),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.fc4 = nn.Sequential(\n",
    "            nn.Linear(2048, self.n_out),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc0(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = x.view(-1, 1, 64, 64)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.n_in = 64*64*1\n",
    "        self.n_out = 1\n",
    "        self.fc0 = nn.Sequential(\n",
    "            nn.Linear(self.n_in, 2048),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.fc4 = nn.Sequential(\n",
    "            nn.Linear(256, self.n_out),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.n_in)\n",
    "        x = self.fc0(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(ngpu)\n",
    "discriminator = Discriminator(ngpu)\n",
    "\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    generator = nn.DataParallel(generator, list(range(ngpu)))\n",
    "    discriminator = nn.DataParallel(discriminator, list(range(ngpu)))\n",
    "\n",
    "g_optim = optim.Adam(generator.parameters(), lr=lr)\n",
    "d_optim = optim.Adam(discriminator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(n, n_features = nz):\n",
    "    return Variable(torch.randn(n, n_features)).to(device)\n",
    "\n",
    "def make_ones(size):\n",
    "    return Variable(torch.ones(size, 1)).to(device)\n",
    "\n",
    "def make_zeros(size):\n",
    "    return Variable(torch.zeros(size, 1)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(optimizer, real_data, fake_data):\n",
    "    n = real_data.size(0)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    prediction_real = discriminator(real_data)\n",
    "    error_real = criterion(prediction_real, make_ones(n))\n",
    "    error_real.backward()\n",
    "\n",
    "    prediction_fake = discriminator(fake_data)\n",
    "    error_fake = criterion(prediction_fake, make_zeros(n))\n",
    "    error_fake.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return error_real + error_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(optimizer, fake_data):\n",
    "    n  = fake_data.size(0)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    prediction = discriminator(fake_data)\n",
    "    error = criterion(prediction, make_ones(n))\n",
    "\n",
    "    error.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_noise = noise(64)\n",
    "generator.train()\n",
    "discriminator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_losses = []\n",
    "d_losses = []\n",
    "images = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    g_error = 0.0\n",
    "    d_error = 0.0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        imgs = data[0]\n",
    "        n = len(imgs)\n",
    "        for j in range(k):\n",
    "            fake_data = generator(noise(n)).detach()\n",
    "            real_data = imgs.to(device)\n",
    "            d_error += train_discriminator(d_optim, real_data, fake_data)\n",
    "        fake_data = generator(noise(n))\n",
    "        g_error += train_generator(g_optim, fake_data)\n",
    "        if i % 10 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_Disscriminator: %.4f\\tLoss_Generator: %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader), (d_error/i).item(), (g_error/i).item()))\n",
    "\n",
    "    img = generator(test_noise).cpu().detach()\n",
    "    img = make_grid(img)\n",
    "    images.append(img)\n",
    "    g_losses.append((g_error/i).item())\n",
    "    d_losses.append((d_error/i).item())\n",
    "    print('Epoch {}: g_loss: {:.8f} d_loss: {:.8f}\\r'.format(epoch, g_error/i, d_error/i))\n",
    "\n",
    "print('Training Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(g_losses, label='Generator_Losses')\n",
    "plt.plot(d_losses, label='Discriminator Losses')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../nets'):\n",
    "    os.mkdir('../nets')\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "torch.save(generator.state_dict(), f'../nets/vanilla_netD_{format(timestamp)}')\n",
    "torch.save(discriminator.state_dict(), f'../nets/vanilla_netG_{format(timestamp)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../prog_gifs'):\n",
    "    os.mkdir('../prog_gifs')\n",
    "imgs = [np.array(to_image(i)) for i in images]\n",
    "imageio.mimsave(f'../prog_gifs/vanilla_progress_{format(timestamp)}.gif', imgs, duration=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../loss'):\n",
    "    os.mkdir('../loss')\n",
    "np.savetxt(f'../loss/vanilla_netD_{format(timestamp)}.txt', np.array(d_losses))\n",
    "np.savetxt(f'../loss/vanilla_netG_{format(timestamp)}.txt', np.array(g_losses))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5405968b28cf7165efd2cf9f73ae0f12240fe0a8206c87e0dd6ec3cb0842cf17"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dl_proj_gan')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
